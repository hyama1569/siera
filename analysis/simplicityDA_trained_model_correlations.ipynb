{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYq3IcL2G96B"
      },
      "source": [
        "# install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lFmRrZB8tD3",
        "outputId": "14befde3-1726-4c10-b0bc-59587c36476d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/Colab Notebooks/master_thesis\n",
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "#!pip install git+https://github.com/facebookresearch/text-simplification-evaluation.git\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "#from torch.utils.data.sampler import BatchSampler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tqdm\n",
        "import itertools\n",
        "import pickle\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore, spearmanr, pearsonr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjMU5NRfHTpk"
      },
      "source": [
        "# margin ranking model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMYv7juGHaU8"
      },
      "outputs": [],
      "source": [
        "from ast import Add\n",
        "from cProfile import label\n",
        "import os\n",
        "from random import shuffle\n",
        "#import hydra\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#from omegaconf import DictConfig\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchmetrics.functional import accuracy, auroc\n",
        "from transformers import BertModel, BertTokenizer\n",
        "#from tseval.feature_extraction import get_compression_ratio, get_wordrank_score\n",
        "import spacy\n",
        "import tqdm\n",
        "#import nltk\n",
        "#nltk.doenload('punkt')\n",
        "\n",
        "\n",
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        tokenizer: BertTokenizer,\n",
        "        max_token_len: int,\n",
        "        orig_column_name: str,\n",
        "        simp_column_name: str,\n",
        "        sent_id_column_name: str,\n",
        "        sys_name_column_name: str,\n",
        "    ):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_len = max_token_len\n",
        "        self.orig_column_name = orig_column_name\n",
        "        self.simp_column_name = simp_column_name\n",
        "        self.sent_id_column_name = sent_id_column_name\n",
        "        self.sys_name_column_name = sys_name_column_name\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_row = self.data.iloc[index]\n",
        "        orig = data_row[self.orig_column_name]\n",
        "        simp = data_row[self.simp_column_name]\n",
        "        sent_id = data_row[self.sent_id_column_name]\n",
        "        sys_name = data_row[self.sys_name_column_name]\n",
        "\n",
        "        encoding_orig_orig = self.tokenizer.encode_plus(\n",
        "            #origin,\n",
        "            orig,\n",
        "            orig,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        encoding_orig_simp = self.tokenizer.encode_plus(\n",
        "            #origin,\n",
        "            orig,\n",
        "            simp,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        encoding_simp_simp = self.tokenizer.encode_plus(\n",
        "            #origin,\n",
        "            simp,\n",
        "            simp,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        encoding_simp_orig = self.tokenizer.encode_plus(\n",
        "            #origin,\n",
        "            simp,\n",
        "            orig,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return dict(\n",
        "            orig_orig=dict(\n",
        "                input_ids=encoding_orig_orig[\"input_ids\"].flatten(),\n",
        "                attention_mask=encoding_orig_orig[\"attention_mask\"].flatten(),\n",
        "                #added_features=orig_orig_added_features_tensor.flatten(),\n",
        "            ),\n",
        "            orig_simp=dict(\n",
        "                input_ids=encoding_orig_simp[\"input_ids\"].flatten(),\n",
        "                attention_mask=encoding_orig_simp[\"attention_mask\"].flatten(),\n",
        "                #added_features=orig_simp_added_features_tensor.flatten(),\n",
        "                sents=simp\n",
        "            ),\n",
        "            simp_simp=dict(\n",
        "                input_ids=encoding_simp_simp[\"input_ids\"].flatten(),\n",
        "                attention_mask=encoding_simp_simp[\"attention_mask\"].flatten(),\n",
        "                #added_features=orig_simp_added_features_tensor.flatten(),\n",
        "                sents=simp\n",
        "            ),\n",
        "            simp_orig=dict(\n",
        "                input_ids=encoding_simp_orig[\"input_ids\"].flatten(),\n",
        "                attention_mask=encoding_simp_orig[\"attention_mask\"].flatten(),\n",
        "                #added_features=orig_simp_added_features_tensor.flatten(),\n",
        "            ),\n",
        "            sent_ids=sent_id,\n",
        "            sys_names=sys_name,\n",
        "            #labels=torch.tensor(label),\n",
        "            #case_nums=torch.tensor(case_num),\n",
        "        )\n",
        "\n",
        "class CreateDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df: pd.DataFrame=None,\n",
        "        valid_df: pd.DataFrame=None,\n",
        "        test_df: pd.DataFrame=None,\n",
        "        batch_size: int=None,\n",
        "        max_token_len: int=None,\n",
        "        #origin_column_name: str = 'origin',\n",
        "        orig_column_name: str = 'orig_sent',\n",
        "        simp_column_name: str = 'simp_sent',\n",
        "        sent_id_column_name: str = 'sent_id',\n",
        "        sys_name_column_name: str = 'sys_name',\n",
        "        #label_column_name: str = 'label',\n",
        "        #case_num_column_name: str = 'case_number',\n",
        "        pretrained_model='bert-base-uncased',\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.valid_df = valid_df\n",
        "        self.test_df = test_df\n",
        "        self.batch_size = batch_size\n",
        "        self.max_token_len = max_token_len\n",
        "        #self.origin_column_name = origin_column_name\n",
        "        self.orig_column_name = orig_column_name\n",
        "        self.simp_column_name = simp_column_name\n",
        "        self.sent_id_column_name = sent_id_column_name\n",
        "        self.sys_name_column_name = sys_name_column_name\n",
        "        #self.label_column_name = label_column_name\n",
        "        #self.case_num_column_name = case_num_column_name\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "    def setup(self, stage):\n",
        "        if stage == \"test\":\n",
        "          self.test_dataset = AugmentedDataset(\n",
        "              self.test_df,\n",
        "              self.tokenizer,\n",
        "              self.max_token_len,\n",
        "              #self.origin_column_name,\n",
        "              self.orig_column_name,\n",
        "              self.simp_column_name,\n",
        "              self.sent_id_column_name,\n",
        "              self.sys_name_column_name,\n",
        "              #self.label_column_name,\n",
        "              #self.case_num_column_name,\n",
        "            )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=os.cpu_count())\n",
        "\n",
        "class BertRanker(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes: int,\n",
        "        learning_rate: float,\n",
        "        pooling_type: str,\n",
        "        added_feature_num: int,\n",
        "        pretrained_model='bert-base-uncased',\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model, output_hidden_states=True, return_dict=True)\n",
        "        classifier_hidden_size = self.bert.config.hidden_size\n",
        "        self.classifier = nn.Linear(classifier_hidden_size + added_feature_num, n_classes)\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.criterion = nn.MarginRankingLoss(margin=1.0)\n",
        "        self.n_classes = n_classes\n",
        "        self.pooling_type = pooling_type\n",
        "\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.bert.encoder.layer[-1].parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        if self.pooling_type == 'cls':\n",
        "            cls = output.pooler_output\n",
        "            #output = torch.cat([added_features.float(), cls], dim=1)\n",
        "            preds = self.classifier(cls)\n",
        "        #preds = torch.flatten(preds)\n",
        "        return preds, output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0afEHd2HfKP"
      },
      "source": [
        "# prepare eval data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RShLmyEiHh_W"
      },
      "outputs": [],
      "source": [
        "with open('./src/simplicityDA_silver_test.pickle', 'rb') as f:\n",
        "    df_simplicityDA = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIlncB28Hz7x"
      },
      "source": [
        "# setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFNabhfoH1A2"
      },
      "outputs": [],
      "source": [
        "max_token_len=512\n",
        "learning_rate=0.05\n",
        "pooling_type = 'cls'\n",
        "added_feature_num = 0\n",
        "feature_path = 'wo_features/'\n",
        "metrics_segment_path = 'metrics_segment_c/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9XfsKslH3By"
      },
      "source": [
        "# evaluation (human eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M1u4lJ5H5sC",
        "outputId": "e0f1a72a-7567-4fb5-bbf2-8c217f215f96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=31.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "1it [00:10, 10.58s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=30.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "2it [00:20,  9.95s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=30 (1).ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "3it [00:30, 10.04s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=29.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "4it [00:40, 10.10s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=25.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "5it [00:49,  9.85s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=25 (1).ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "6it [01:00, 10.06s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=24.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "7it [01:10, 10.07s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=23.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "8it [01:19,  9.86s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=23 (1).ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "9it [01:30, 10.09s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/base/100/m=1/epoch=22.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "10it [01:39,  9.91s/it]\n",
            "0it [00:00, ?it/s]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=18 (1).ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "1it [00:09,  9.79s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=19 (1).ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "2it [00:20, 10.18s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=21 (2).ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "3it [00:29,  9.57s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=18.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "4it [00:39,  9.88s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=19.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "5it [00:48,  9.73s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=25.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "6it [00:57,  9.51s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=17.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "7it [01:08,  9.82s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=20.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "8it [01:17,  9.51s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=21 (1).ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "9it [01:26,  9.49s/it]INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.1 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/wo_features/exp2/silver/100/m=1/epoch=21.ckpt`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start prediction of mr model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-469726bc9c85>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
            "<ipython-input-13-469726bc9c85>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
            "10it [01:37,  9.70s/it]\n"
          ]
        }
      ],
      "source": [
        "ckpaths = ['exp2/base/100/m=1/epoch=', 'exp2/silver/100/m=1/epoch=']\n",
        "epoch_lists = [['47', '42', '41', '31', '29', '28', '28 (1)', '22', '21', '19'], ['46', '37', '37 (1)', '33', '33 (1)', '30', '26', '26 (1)', '23', '23 (1)']] #v11_nprm_filtered_simplicityDA\n",
        "epoch_lists = [['31', '30', '30 (1)', '29', '25', '25 (1)', '24', '23', '23 (1)', '22'], ['18 (1)', '19 (1)', '21 (2)', '18', '19', '25', '17', '20', '21 (1)', '21']] #v11_nprm_filtered_simplicityDA_trial2\n",
        "\n",
        "for ckpath, epoch_list in zip(ckpaths, epoch_lists):\n",
        "    df_metrics_segment_DA = pd.DataFrame()\n",
        "    df_metrics_segment_hg = pd.DataFrame()\n",
        "    df_metrics_segment_simpDA = pd.DataFrame()\n",
        "    for i, epoch_num in tqdm.tqdm(enumerate(epoch_list)):\n",
        "        now_model = 'model_' + str(i)\n",
        "        model = BertRanker.load_from_checkpoint(\n",
        "            n_classes=2,\n",
        "            learning_rate=learning_rate,\n",
        "            pooling_type=pooling_type,\n",
        "            added_feature_num = added_feature_num,\n",
        "            checkpoint_path='./lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/' + feature_path + ckpath + epoch_num + '.ckpt',\n",
        "        )\n",
        "        #model = nn.DataParallel(model, device_ids=[2,3])\n",
        "        device = 'cuda'\n",
        "        model.to(device)\n",
        "\n",
        "        data_module = CreateDataModule(\n",
        "        test_df=df_simplicityDA,\n",
        "        batch_size=1,\n",
        "        max_token_len=max_token_len,\n",
        "        )\n",
        "        data_module.setup(stage='test')\n",
        "        test_dataloader = data_module.test_dataloader()\n",
        "        metrics_DA = []\n",
        "        sent_ids = []\n",
        "        sys_names = []\n",
        "        test_sets = []\n",
        "        with torch.no_grad():\n",
        "            print(\"start prediction of mr model\")\n",
        "            for batch in test_dataloader:\n",
        "                input_ids=batch[\"orig_simp\"][\"input_ids\"]\n",
        "                attention_mask=batch[\"orig_simp\"][\"attention_mask\"]\n",
        "\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "\n",
        "                preds_orig_simp, output = model(input_ids, attention_mask)\n",
        "                preds_orig_simp = torch.nn.functional.softmax(preds_orig_simp)\n",
        "\n",
        "                input_ids=batch[\"simp_orig\"][\"input_ids\"]\n",
        "                attention_mask=batch[\"simp_orig\"][\"attention_mask\"]\n",
        "\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "\n",
        "                preds_simp_orig, output = model(input_ids, attention_mask)\n",
        "                preds_simp_orig = torch.nn.functional.softmax(preds_simp_orig)\n",
        "\n",
        "                #metrics_DA.append(preds_orig_simp[0].item() - preds_orig_orig[0].item())\n",
        "                #metrics_DA.append(preds_orig_simp[0][1].item()) # pattern a\n",
        "                #metrics_DA.append(preds_simp_orig[0][0].item()) # pattern b\n",
        "                metrics_DA.append((preds_orig_simp[0][1].item() + preds_simp_orig[0][0].item()) / 2) # pattern c\n",
        "                #metrics_DA.append(min(preds_orig_simp[0][1].item(), preds_simp_orig[0][0].item())) # pattern f\n",
        "                #metrics_DA.append(max(preds_orig_simp[0][1].item(), preds_simp_orig[0][0].item())) # pattern g\n",
        "                sent_ids.append(int(batch['sent_ids'][0]))\n",
        "                sys_names.append(batch['sys_names'][0])\n",
        "                test_sets.append('asset')\n",
        "\n",
        "        if len(df_metrics_segment_DA) != 0:\n",
        "            df_metrics_segment_DA = df_metrics_segment_DA.merge(pd.DataFrame({'sent_id':sent_ids, 'sys_name':sys_names, 'test_set':test_sets, now_model:metrics_DA}), on=['sent_id', 'sys_name', 'test_set'])\n",
        "        else:\n",
        "            df_metrics_segment_DA = pd.DataFrame({'sent_id':sent_ids, 'sys_name':sys_names, 'test_set':test_sets, now_model:metrics_DA})\n",
        "\n",
        "    save_name = feature_path.replace('/', '_') + ckpath[:-7].replace('/', '_')\n",
        "\n",
        "    df_metrics_segment_DA_save = df_metrics_segment_DA[['sent_id', 'sys_name', 'test_set']].copy()\n",
        "    df_metrics_segment_DA_save[save_name] = df_metrics_segment_DA.drop(columns=['sent_id', 'sys_name', 'test_set']).mean(axis=1)\n",
        "    df_metrics_segment_DA_save = df_metrics_segment_DA_save.merge(df_metrics_segment_DA, on=['sent_id', 'sys_name', 'test_set'])\n",
        "    with open('./lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/' + feature_path + ckpath[:5] + metrics_segment_path + save_name + '_DA'  + '.pickle', 'wb') as f:\n",
        "        pickle.dump(df_metrics_segment_DA_save, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p95GTAFgMFUv"
      },
      "source": [
        "# correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziK5Jo9mMHpy"
      },
      "source": [
        "## import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pNSax0rMGwa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore, pearsonr, spearmanr, t, kendalltau\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "def compute_direct_assessment_correlations(df_human_scores, df_metrics_scores, aspect, segment_id_cols,\n",
        "                                           use_absolute_values=True):\n",
        "    df_da_scores = df_human_scores.reset_index()\n",
        "    cols_of_interest = segment_id_cols + [f\"{aspect}_zscore\"]\n",
        "    df_da_scores = df_da_scores[cols_of_interest]\n",
        "    df_all_scores = pd.merge(left=df_metrics_scores, right=df_da_scores, on=segment_id_cols)\n",
        "\n",
        "    # Compute correlations metrics vs human scores\n",
        "    #print(\"Computing correlations...\")\n",
        "    metrics_names = [col for col in df_metrics_scores.columns if col not in segment_id_cols]\n",
        "    correlations_data = []\n",
        "    for metric in metrics_names:\n",
        "        corr, p_value = pearsonr(df_all_scores[metric], df_all_scores[f'{aspect}_zscore'])\n",
        "        if use_absolute_values:\n",
        "            corr = abs(corr)\n",
        "        correlations_data.append([metric, corr, p_value])\n",
        "    df_correlations_metrics_human = pd.DataFrame(correlations_data, columns=['metric', 'corr', 'p_value'])\n",
        "    df_correlations_metrics_human.sort_values(by=['corr'], ascending=False, inplace=True, ignore_index=True)\n",
        "\n",
        "    # Compute correlations metrics vs metrics\n",
        "    metrics_names = df_correlations_metrics_human['metric'].to_list()\n",
        "    correlations_data = []\n",
        "    for _, (metric_a, corr_metric_a, _) in df_correlations_metrics_human.iterrows():\n",
        "        for _, (metric_b, corr_metric_b, _) in df_correlations_metrics_human.iterrows():\n",
        "            corr_a_b, pvalue_a_b = pearsonr(df_all_scores[metric_a], df_all_scores[metric_b])\n",
        "            if use_absolute_values:\n",
        "                corr_a_b = abs(corr_a_b)\n",
        "            correlations_data.append([metric_a, corr_metric_a,\n",
        "                                      metric_b, corr_metric_b,\n",
        "                                      corr_a_b, pvalue_a_b])\n",
        "    df_correlations_metric_metric = pd.DataFrame(correlations_data,\n",
        "                                                 columns=['metric_a', 'corr_metric_a',\n",
        "                                                          'metric_b', 'corr_metric_b',\n",
        "                                                          'corr_a_b', 'pvalue_a_b'])\n",
        "\n",
        "    # Determine if the difference in performance is significant\n",
        "    #print(\"Determining if the difference in performance is significant...\")\n",
        "    significance_matrix = []\n",
        "    winner_status = []\n",
        "    for metric_a in metrics_names:\n",
        "        df_correlations = df_correlations_metric_metric[df_correlations_metric_metric['metric_a'] == metric_a]\n",
        "        is_winner = True\n",
        "        significance_row = []\n",
        "        for _, (_, corr_metric_a, metric_b, corr_metric_b, corr_a_b, _) in df_correlations.iterrows():\n",
        "            p = np.nan\n",
        "            if (metric_a != metric_b) and (corr_metric_a > corr_metric_b):\n",
        "                _, p = williams_test(corr_metric_a, corr_metric_b, corr_a_b, len(df_human_scores))\n",
        "            is_diff_stats_significant = p < 0.05\n",
        "            if not is_diff_stats_significant:\n",
        "                # we do not care about the exact values in cases where it's not significant\n",
        "                p = np.nan\n",
        "            significance_row.append(p)\n",
        "            # Update winner status (not significantly outperformed by any other metric)\n",
        "            if metric_a != metric_b:\n",
        "                is_winner = is_winner and is_diff_stats_significant\n",
        "        significance_matrix.append(significance_row)\n",
        "        winner_status.append(is_winner)\n",
        "    df_correlations_metrics_human['is_winner'] = winner_status\n",
        "    df_significance = pd.DataFrame(np.array(significance_matrix), columns=metrics_names, index=metrics_names)\n",
        "\n",
        "    return df_correlations_metrics_human, df_significance\n",
        "\n",
        "def compute_relative_ranking_correlations(df_human_scores, df_metrics_scores, aspect, segment_id_cols,\n",
        "                                           use_absolute_values=True):\n",
        "    df_da_scores = df_human_scores.reset_index()\n",
        "    cols_of_interest = segment_id_cols + [f\"{aspect}_zscore\"]\n",
        "    df_da_scores = df_da_scores[cols_of_interest]\n",
        "    df_all_scores = pd.merge(left=df_metrics_scores, right=df_da_scores, on=segment_id_cols)\n",
        "\n",
        "    # Compute correlations metrics vs human scores\n",
        "    #print(\"Computing correlations...\")\n",
        "    metrics_names = [col for col in df_metrics_scores.columns if col not in segment_id_cols]\n",
        "    correlations_data = []\n",
        "    for metric in metrics_names:\n",
        "        corr, p_value = spearmanr(df_all_scores[metric], df_all_scores[f'{aspect}_zscore'])\n",
        "        if use_absolute_values:\n",
        "            corr = abs(corr)\n",
        "        correlations_data.append([metric, corr, p_value])\n",
        "    df_correlations_metrics_human = pd.DataFrame(correlations_data, columns=['metric', 'corr', 'p_value'])\n",
        "    df_correlations_metrics_human.sort_values(by=['corr'], ascending=False, inplace=True, ignore_index=True)\n",
        "\n",
        "    # Compute correlations metrics vs metrics\n",
        "    metrics_names = df_correlations_metrics_human['metric'].to_list()\n",
        "    correlations_data = []\n",
        "    for _, (metric_a, corr_metric_a, _) in df_correlations_metrics_human.iterrows():\n",
        "        for _, (metric_b, corr_metric_b, _) in df_correlations_metrics_human.iterrows():\n",
        "            corr_a_b, pvalue_a_b = spearmanr(df_all_scores[metric_a], df_all_scores[metric_b])\n",
        "            if use_absolute_values:\n",
        "                corr_a_b = abs(corr_a_b)\n",
        "            correlations_data.append([metric_a, corr_metric_a,\n",
        "                                      metric_b, corr_metric_b,\n",
        "                                      corr_a_b, pvalue_a_b])\n",
        "    df_correlations_metric_metric = pd.DataFrame(correlations_data,\n",
        "                                                 columns=['metric_a', 'corr_metric_a',\n",
        "                                                          'metric_b', 'corr_metric_b',\n",
        "                                                          'corr_a_b', 'pvalue_a_b'])\n",
        "\n",
        "    # Determine if the difference in performance is significant\n",
        "    #print(\"Determining if the difference in performance is significant...\")\n",
        "    significance_matrix = []\n",
        "    winner_status = []\n",
        "    for metric_a in metrics_names:\n",
        "        df_correlations = df_correlations_metric_metric[df_correlations_metric_metric['metric_a'] == metric_a]\n",
        "        is_winner = True\n",
        "        significance_row = []\n",
        "        for _, (_, corr_metric_a, metric_b, corr_metric_b, corr_a_b, _) in df_correlations.iterrows():\n",
        "            p = np.nan\n",
        "            if (metric_a != metric_b) and (corr_metric_a > corr_metric_b):\n",
        "                _, p = williams_test(corr_metric_a, corr_metric_b, corr_a_b, len(df_human_scores))\n",
        "            is_diff_stats_significant = p < 0.05\n",
        "            if not is_diff_stats_significant:\n",
        "                # we do not care about the exact values in cases where it's not significant\n",
        "                p = np.nan\n",
        "            significance_row.append(p)\n",
        "            # Update winner status (not significantly outperformed by any other metric)\n",
        "            if metric_a != metric_b:\n",
        "                is_winner = is_winner and is_diff_stats_significant\n",
        "        significance_matrix.append(significance_row)\n",
        "        winner_status.append(is_winner)\n",
        "    df_correlations_metrics_human['is_winner'] = winner_status\n",
        "    df_significance = pd.DataFrame(np.array(significance_matrix), columns=metrics_names, index=metrics_names)\n",
        "\n",
        "    return df_correlations_metrics_human, df_significance\n",
        "\n",
        "\n",
        "def compute_kendall_tau_correlations(df_human_scores, df_metrics_scores, aspect, segment_id_cols,\n",
        "                                           use_absolute_values=True):\n",
        "    df_da_scores = df_human_scores.reset_index()\n",
        "    cols_of_interest = segment_id_cols + [f\"{aspect}_zscore\"]\n",
        "    df_da_scores = df_da_scores[cols_of_interest]\n",
        "    df_all_scores = pd.merge(left=df_metrics_scores, right=df_da_scores, on=segment_id_cols)\n",
        "\n",
        "    # Compute correlations metrics vs human scores\n",
        "    #print(\"Computing correlations...\")\n",
        "    metrics_names = [col for col in df_metrics_scores.columns if col not in segment_id_cols]\n",
        "    correlations_data = []\n",
        "    for metric in metrics_names:\n",
        "        corr, p_value = kendalltau(df_all_scores[metric], df_all_scores[f'{aspect}_zscore'])\n",
        "        if use_absolute_values:\n",
        "            corr = abs(corr)\n",
        "        correlations_data.append([metric, corr, p_value])\n",
        "    df_correlations_metrics_human = pd.DataFrame(correlations_data, columns=['metric', 'corr', 'p_value'])\n",
        "    df_correlations_metrics_human.sort_values(by=['corr'], ascending=False, inplace=True, ignore_index=True)\n",
        "\n",
        "    # Compute correlations metrics vs metrics\n",
        "    metrics_names = df_correlations_metrics_human['metric'].to_list()\n",
        "    correlations_data = []\n",
        "    for _, (metric_a, corr_metric_a, _) in df_correlations_metrics_human.iterrows():\n",
        "        for _, (metric_b, corr_metric_b, _) in df_correlations_metrics_human.iterrows():\n",
        "            corr_a_b, pvalue_a_b = kendalltau(df_all_scores[metric_a], df_all_scores[metric_b])\n",
        "            if use_absolute_values:\n",
        "                corr_a_b = abs(corr_a_b)\n",
        "            correlations_data.append([metric_a, corr_metric_a,\n",
        "                                      metric_b, corr_metric_b,\n",
        "                                      corr_a_b, pvalue_a_b])\n",
        "    df_correlations_metric_metric = pd.DataFrame(correlations_data,\n",
        "                                                 columns=['metric_a', 'corr_metric_a',\n",
        "                                                          'metric_b', 'corr_metric_b',\n",
        "                                                          'corr_a_b', 'pvalue_a_b'])\n",
        "\n",
        "    # Determine if the difference in performance is significant\n",
        "    #print(\"Determining if the difference in performance is significant...\")\n",
        "    significance_matrix = []\n",
        "    winner_status = []\n",
        "    for metric_a in metrics_names:\n",
        "        df_correlations = df_correlations_metric_metric[df_correlations_metric_metric['metric_a'] == metric_a]\n",
        "        is_winner = True\n",
        "        significance_row = []\n",
        "        for _, (_, corr_metric_a, metric_b, corr_metric_b, corr_a_b, _) in df_correlations.iterrows():\n",
        "            p = np.nan\n",
        "            if (metric_a != metric_b) and (corr_metric_a > corr_metric_b):\n",
        "                _, p = williams_test(corr_metric_a, corr_metric_b, corr_a_b, len(df_human_scores))\n",
        "            is_diff_stats_significant = p < 0.05\n",
        "            if not is_diff_stats_significant:\n",
        "                # we do not care about the exact values in cases where it's not significant\n",
        "                p = np.nan\n",
        "            significance_row.append(p)\n",
        "            # Update winner status (not significantly outperformed by any other metric)\n",
        "            if metric_a != metric_b:\n",
        "                is_winner = is_winner and is_diff_stats_significant\n",
        "        significance_matrix.append(significance_row)\n",
        "        winner_status.append(is_winner)\n",
        "    df_correlations_metrics_human['is_winner'] = winner_status\n",
        "    df_significance = pd.DataFrame(np.array(significance_matrix), columns=metrics_names, index=metrics_names)\n",
        "\n",
        "    return df_correlations_metrics_human, df_significance\n",
        "\n",
        "\n",
        "# From https://github.com/inmoonlight/nlp-williams/blob/master/williams.py\n",
        "def williams_test(r12, r13, r23, n):\n",
        "\n",
        "    \"\"\"The Williams test (Evan J. Williams. 1959. Regression Analysis, volume 14. Wiley, New York, USA)\n",
        "    A test of whether the population correlation r12 equals the population correlation r13.\n",
        "    Significant: p < 0.05\n",
        "    Arguments:\n",
        "        r12 (float): correlation between x1, x2\n",
        "        r13 (float): correlation between x1, x3\n",
        "        r23 (float): correlation between x2, x3\n",
        "        n (int): size of the population\n",
        "    Returns:\n",
        "        t (float): Williams test result\n",
        "        p (float): p-value of t-dist\n",
        "    \"\"\"\n",
        "    assert (r12 >= r13), \"r12 should be larger than r13\"\n",
        "    assert (n > 3), \"n should be larger than 3\"\n",
        "\n",
        "    K = 1 - r12 ** 2 - r13 ** 2 - r23 ** 2 + 2 * r12 * r13 * r23\n",
        "    denominator = np.sqrt(\n",
        "        2 * K * (n - 1) / (n - 3) + (((r12 + r13) ** 2) / 4) * ((1 - r23) ** 3)\n",
        "    )\n",
        "    numerator = (r12 - r13) * np.sqrt((n - 1) * (1 + r23))\n",
        "    p = 1 - t.cdf(numerator / denominator, df=n - 3)  # changed to n-3 on 30/11/14\n",
        "    return t, p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Pf673lMNL5"
      },
      "outputs": [],
      "source": [
        "def compute_direct_correlations(df_benchmark, df_segment_metrics, aspects, test_sets, segment_id_cols):\n",
        "    results = {}\n",
        "    for aspect in aspects:\n",
        "        smed = df_benchmark['simplicity_zscore'].median()\n",
        "        shind = df_benchmark['simplicity_zscore'] > smed\n",
        "        slind = df_benchmark['simplicity_zscore'] <= smed\n",
        "        fmed = df_benchmark['fluency_zscore'].median()\n",
        "        fhind = df_benchmark['fluency_zscore'] > fmed\n",
        "        flind = df_benchmark['fluency_zscore'] <= fmed\n",
        "        mmed = df_benchmark['meaning_zscore'].median()\n",
        "        mhind = df_benchmark['meaning_zscore'] > mmed\n",
        "        mlind = df_benchmark['meaning_zscore'] <= mmed\n",
        "        fmhind = (df_benchmark['fluency_zscore'] > fmed) & (df_benchmark['meaning_zscore'] > mmed)\n",
        "        fmlind = (df_benchmark['fluency_zscore'] <= fmed) & (df_benchmark['meaning_zscore'] <= mmed)\n",
        "        smfhind = (df_benchmark['simplicity_zscore'] > smed) & (df_benchmark['fluency_zscore'] > fmed) & (df_benchmark['meaning_zscore'] > mmed)\n",
        "        smflind = (df_benchmark['simplicity_zscore'] <= smed) & (df_benchmark['fluency_zscore'] <= fmed) & (df_benchmark['meaning_zscore'] <= mmed)\n",
        "        df_scores_slow = df_benchmark[slind]\n",
        "        df_scores_shigh = df_benchmark[shind]\n",
        "        df_scores_flow = df_benchmark[flind]\n",
        "        df_scores_fhigh = df_benchmark[fhind]\n",
        "        df_scores_mlow = df_benchmark[mlind]\n",
        "        df_scores_mhigh = df_benchmark[mhind]\n",
        "        df_scores_fmlow = df_benchmark[fmlind]\n",
        "        df_scores_fmhigh = df_benchmark[fmhind]\n",
        "        df_scores_sfmlow = df_benchmark[smflind]\n",
        "        df_scores_sfmhigh = df_benchmark[smfhind]\n",
        "\n",
        "        print(f\"{aspect}: simp_high ({len(df_scores_shigh)}) - simp_low ({len(df_scores_slow)}) - flu_low ({len(df_scores_flow)}) - flu_high ({len(df_scores_fhigh)}) - mean_low ({len(df_scores_mlow)}) - mean_high ({len(df_scores_mhigh)}) - fm_low ({len(df_scores_fmlow)}) - fm_high ({len(df_scores_fmhigh)}) - sfm_low ({len(df_scores_sfmlow)}) - sfm_high ({len(df_scores_sfmhigh)}) - All ({len(df_benchmark)})\")\n",
        "        for quality, df_scores in {'simp_low': df_scores_slow, 'simp_high': df_scores_shigh, 'flu_low': df_scores_flow, 'flu_high': df_scores_fhigh, 'mean_low': df_scores_mlow, 'mean_high': df_scores_mhigh, 'fm_low': df_scores_fmlow, 'fm_high': df_scores_fmhigh, 'sfm_low': df_scores_sfmlow, 'sfm_high': df_scores_sfmhigh, 'all': df_benchmark}.items():\n",
        "            for test_set in test_sets:\n",
        "                #print(f\"Computing for {quality} scores - {test_set} references\")\n",
        "                df_metrics = df_segment_metrics[df_segment_metrics.test_set==test_set].drop(columns=['test_set'])\n",
        "                results[(quality, aspect, test_set)] = compute_direct_assessment_correlations(\n",
        "                    df_scores,\n",
        "                    df_metrics,\n",
        "                    aspect,\n",
        "                    segment_id_cols=segment_id_cols,\n",
        "                    use_absolute_values=False\n",
        "                )\n",
        "                #print()\n",
        "    return results\n",
        "\n",
        "def compute_ranking_correlations(df_benchmark, df_segment_metrics, aspects, test_sets, segment_id_cols):\n",
        "    results = {}\n",
        "    for aspect in aspects:\n",
        "        smed = df_benchmark['simplicity_zscore'].median()\n",
        "        shind = df_benchmark['simplicity_zscore'] > smed\n",
        "        slind = df_benchmark['simplicity_zscore'] <= smed\n",
        "        fmed = df_benchmark['fluency_zscore'].median()\n",
        "        fhind = df_benchmark['fluency_zscore'] > fmed\n",
        "        flind = df_benchmark['fluency_zscore'] <= fmed\n",
        "        mmed = df_benchmark['meaning_zscore'].median()\n",
        "        mhind = df_benchmark['meaning_zscore'] > mmed\n",
        "        mlind = df_benchmark['meaning_zscore'] <= mmed\n",
        "        fmhind = (df_benchmark['fluency_zscore'] > fmed) & (df_benchmark['meaning_zscore'] > mmed)\n",
        "        fmlind = (df_benchmark['fluency_zscore'] <= fmed) & (df_benchmark['meaning_zscore'] <= mmed)\n",
        "        smfhind = (df_benchmark['simplicity_zscore'] > smed) & (df_benchmark['fluency_zscore'] > fmed) & (df_benchmark['meaning_zscore'] > mmed)\n",
        "        smflind = (df_benchmark['simplicity_zscore'] <= smed) & (df_benchmark['fluency_zscore'] <= fmed) & (df_benchmark['meaning_zscore'] <= mmed)\n",
        "        df_scores_slow = df_benchmark[slind]\n",
        "        df_scores_shigh = df_benchmark[shind]\n",
        "        df_scores_flow = df_benchmark[flind]\n",
        "        df_scores_fhigh = df_benchmark[fhind]\n",
        "        df_scores_mlow = df_benchmark[mlind]\n",
        "        df_scores_mhigh = df_benchmark[mhind]\n",
        "        df_scores_fmlow = df_benchmark[fmlind]\n",
        "        df_scores_fmhigh = df_benchmark[fmhind]\n",
        "        df_scores_sfmlow = df_benchmark[smflind]\n",
        "        df_scores_sfmhigh = df_benchmark[smfhind]\n",
        "\n",
        "        print(f\"{aspect}: simp_high ({len(df_scores_shigh)}) - simp_low ({len(df_scores_slow)}) - flu_low ({len(df_scores_flow)}) - flu_high ({len(df_scores_fhigh)}) - mean_low ({len(df_scores_mlow)}) - mean_high ({len(df_scores_mhigh)}) - fm_low ({len(df_scores_fmlow)}) - fm_high ({len(df_scores_fmhigh)}) - sfm_low ({len(df_scores_sfmlow)}) - sfm_high ({len(df_scores_sfmhigh)}) - All ({len(df_benchmark)})\")\n",
        "        for quality, df_scores in {'simp_low': df_scores_slow, 'simp_high': df_scores_shigh, 'flu_low': df_scores_flow, 'flu_high': df_scores_fhigh, 'mean_low': df_scores_mlow, 'mean_high': df_scores_mhigh, 'fm_low': df_scores_fmlow, 'fm_high': df_scores_fmhigh, 'sfm_low': df_scores_sfmlow, 'sfm_high': df_scores_sfmhigh, 'all': df_benchmark}.items():\n",
        "            for test_set in test_sets:\n",
        "                #print(f\"Computing for {quality} scores - {test_set} references\")\n",
        "                df_metrics = df_segment_metrics[df_segment_metrics.test_set==test_set].drop(columns=['test_set'])\n",
        "                results[(quality, aspect, test_set)] = compute_relative_ranking_correlations(\n",
        "                    df_human_scores=df_scores,\n",
        "                    df_metrics_scores=df_metrics,\n",
        "                    aspect=aspect,\n",
        "                    segment_id_cols=segment_id_cols,\n",
        "                    #sentence_id_cols=['sent_id'],\n",
        "                    #system_id_cols=['sys_name'],\n",
        "                    use_absolute_values=False,\n",
        "                )\n",
        "                #print()\n",
        "    return results\n",
        "\n",
        "def compute_kendall_correlations(df_benchmark, df_segment_metrics, aspects, test_sets, segment_id_cols):\n",
        "    results = {}\n",
        "    for aspect in aspects:\n",
        "        smed = df_benchmark['simplicity_zscore'].median()\n",
        "        shind = df_benchmark['simplicity_zscore'] > smed\n",
        "        slind = df_benchmark['simplicity_zscore'] <= smed\n",
        "        fmed = df_benchmark['fluency_zscore'].median()\n",
        "        fhind = df_benchmark['fluency_zscore'] > fmed\n",
        "        flind = df_benchmark['fluency_zscore'] <= fmed\n",
        "        mmed = df_benchmark['meaning_zscore'].median()\n",
        "        mhind = df_benchmark['meaning_zscore'] > mmed\n",
        "        mlind = df_benchmark['meaning_zscore'] <= mmed\n",
        "        fmhind = (df_benchmark['fluency_zscore'] > fmed) & (df_benchmark['meaning_zscore'] > mmed)\n",
        "        fmlind = (df_benchmark['fluency_zscore'] <= fmed) & (df_benchmark['meaning_zscore'] <= mmed)\n",
        "        df_scores_slow = df_benchmark[slind]\n",
        "        df_scores_shigh = df_benchmark[shind]\n",
        "        df_scores_flow = df_benchmark[flind]\n",
        "        df_scores_fhigh = df_benchmark[fhind]\n",
        "        df_scores_mlow = df_benchmark[mlind]\n",
        "        df_scores_mhigh = df_benchmark[mhind]\n",
        "        df_scores_fmlow = df_benchmark[fmlind]\n",
        "        df_scores_fmhigh = df_benchmark[fmhind]\n",
        "\n",
        "        print(f\"{aspect}: simp_high ({len(df_scores_shigh)}) - simp_low ({len(df_scores_slow)}) - flu_low ({len(df_scores_flow)}) - flu_high ({len(df_scores_fhigh)}) - mean_low ({len(df_scores_mlow)}) - mean_high ({len(df_scores_mhigh)}) - fm_low ({len(df_scores_fmlow)}) - fm_high ({len(df_scores_fmhigh)}) - All ({len(df_benchmark)})\")\n",
        "        for quality, df_scores in {'simp_low': df_scores_slow, 'simp_high': df_scores_shigh, 'flu_low': df_scores_flow, 'flu_high': df_scores_fhigh, 'mean_low': df_scores_mlow, 'mean_high': df_scores_mhigh, 'fm_low': df_scores_fmlow, 'fm_high': df_scores_fmhigh, 'all': df_benchmark}.items():\n",
        "            for test_set in test_sets:\n",
        "                #print(f\"Computing for {quality} scores - {test_set} references\")\n",
        "                df_metrics = df_segment_metrics[df_segment_metrics.test_set==test_set].drop(columns=['test_set'])\n",
        "                results[(quality, aspect, test_set)] = compute_kendall_tau_correlations(\n",
        "                    df_human_scores=df_scores,\n",
        "                    df_metrics_scores=df_metrics,\n",
        "                    aspect=aspect,\n",
        "                    segment_id_cols=segment_id_cols,\n",
        "                    #sentence_id_cols=['sent_id'],\n",
        "                    #system_id_cols=['sys_name'],\n",
        "                    use_absolute_values=False,\n",
        "                )\n",
        "                #print()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AwbhJcMMQLk"
      },
      "outputs": [],
      "source": [
        "with open('./src/simplicityDA_silver_test.pickle', 'rb') as f:\n",
        "    df_simplicityDA = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9foliv1sMX8o"
      },
      "source": [
        "## calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJrsFrKKMaQH"
      },
      "outputs": [],
      "source": [
        "feature_path = 'wo_features/'\n",
        "#metrics_segment_path = 'metrics_segment_a/'\n",
        "#metrics_segment_path = 'metrics_segment_b/'\n",
        "metrics_segment_path = 'metrics_segment_c/'\n",
        "ckpaths = ['exp2/base/100/m=1/epoch=', 'exp2/silver/100/m=1/epoch=']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnfl_KZWOzOU"
      },
      "outputs": [],
      "source": [
        "df_metrics_segment_DA_all = pd.DataFrame()\n",
        "ckpath = ckpaths[1] #change here\n",
        "save_name = feature_path.replace('/', '_') + ckpath[:-7].replace('/', '_')\n",
        "path_DA = './lightning_logs_folder/v11_nprm_filtered_simplicityDA_trial2/' + feature_path + ckpath[:5] + metrics_segment_path + save_name + '_DA'  + '.pickle'\n",
        "with open(path_DA, 'rb') as f:\n",
        "    df_metrics_segment_DA = pickle.load(f)\n",
        "\n",
        "if len(df_metrics_segment_DA_all) != 0:\n",
        "    df_metrics_segment_DA_all = df_metrics_segment_DA_all.merge(df_metrics_segment_DA, on=['sent_id', 'sys_name', 'test_set'])\n",
        "else:\n",
        "    df_metrics_segment_DA_all = df_metrics_segment_DA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAik_1whPIpc",
        "outputId": "acb5de2b-19d0-405c-baeb-89565bae11a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simplicity: simp_high (60) - simp_low (60) - flu_low (60) - flu_high (60) - mean_low (60) - mean_high (60) - fm_low (43) - fm_high (43) - sfm_low (40) - sfm_high (36) - All (120)\n",
            "0.6212664875112741\n",
            "0.025844181946461306\n",
            "[0.660374080045129, 0.6485903193061069, 0.6369042885180044, 0.6330453032550467, 0.6323196256618717, 0.6203657021822395, 0.6084012163026571, 0.6020280060483245, 0.5926950770378598, 0.5779412567555016]\n"
          ]
        }
      ],
      "source": [
        "results_simplicity = compute_direct_correlations(\n",
        "                        df_simplicityDA,\n",
        "                        df_metrics_segment_DA_all,\n",
        "                        aspects=['simplicity'],\n",
        "                        test_sets=['asset'],\n",
        "                        segment_id_cols=['sent_id','sys_name']\n",
        "                    )\n",
        "print(results_simplicity[('all', 'simplicity', 'asset')][0][results_simplicity[('all', 'simplicity', 'asset')][0][\"metric\"] != save_name][\"corr\"].mean())\n",
        "print(results_simplicity[('all', 'simplicity', 'asset')][0][results_simplicity[('all', 'simplicity', 'asset')][0][\"metric\"] != save_name][\"corr\"].std())\n",
        "print(list(results_simplicity[('all', 'simplicity', 'asset')][0][results_simplicity[('all', 'simplicity', 'asset')][0][\"metric\"] != save_name][\"corr\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbC1CsgDQXqB",
        "outputId": "4f9a07a1-c405-4e74-ef26-77df70ea126c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simplicity: simp_high (60) - simp_low (60) - flu_low (60) - flu_high (60) - mean_low (60) - mean_high (60) - fm_low (43) - fm_high (43) - sfm_low (40) - sfm_high (36) - All (120)\n",
            "0.654582957149802\n",
            "0.023380909562837383\n",
            "[0.686756024723939, 0.6839433293978748, 0.6794013473157858, 0.6592332800888949, 0.6539273560663934, 0.6500451420237515, 0.6466629627057434, 0.6402458504062781, 0.628995069102021, 0.6166192096673379]\n"
          ]
        }
      ],
      "source": [
        "results_simplicity = compute_ranking_correlations(\n",
        "                        df_simplicityDA,\n",
        "                        df_metrics_segment_DA_all,\n",
        "                        aspects=['simplicity'],\n",
        "                        test_sets=['asset'],\n",
        "                        segment_id_cols=['sent_id','sys_name']\n",
        "                    )\n",
        "print(results_simplicity[('all', 'simplicity', 'asset')][0][results_simplicity[('all', 'simplicity', 'asset')][0][\"metric\"] != save_name][\"corr\"].mean())\n",
        "print(results_simplicity[('all', 'simplicity', 'asset')][0][results_simplicity[('all', 'simplicity', 'asset')][0][\"metric\"] != save_name][\"corr\"].std())\n",
        "print(list(results_simplicity[('all', 'simplicity', 'asset')][0][results_simplicity[('all', 'simplicity', 'asset')][0][\"metric\"] != save_name][\"corr\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C9tntgNwKpF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
